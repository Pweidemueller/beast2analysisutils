{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to beast2analysisutils","text":"<p>A miscellaneous collection of analysis utilities that I frequently use in the BEAST2 ecosystem.</p> <p>Broadly there are two types of analysis currently in this package:</p> <ol> <li>ReMASTER XML Generation: Construct BEAST2 XML files from templates by inserting simulated trees and/or sequences from ReMASTER output.</li> <li>ESS Analysis: Calculate the effective sample sizes (ESS) for all parameters in a BEAST2 log file.</li> </ol>"},{"location":"#installation","title":"Installation","text":"<p>If you already have a python environment set up, you can install this package via <code>pip</code> directly from GitHub.</p> <p>1. Conda Environment</p> <pre><code>conda activate my_analysis_env\npip install git+https://github.com/Pweidemueller/beast2analysisutils.git\n</code></pre> <p>2. Standard pip/venv</p> <pre><code>source .venv/bin/activate\npip install git+https://github.com/Pweidemueller/beast2analysisutils.git\n</code></pre> <p>3. uv Environment</p> <pre><code>uv pip install git+https://github.com/Pweidemueller/beast2analysisutils.git\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the Reference page for full API details.</p>"},{"location":"#1-beast2-xml-generation-from-remaster-output","title":"1. BEAST2 XML Generation from ReMASTER Output","text":"<p>This module helps you turn ReMASTER simulation outputs (Nexus alignment + Tree) into runnable BEAST2 XMLs given a template XML.</p>"},{"location":"#step-1-construct-templates","title":"Step 1: Construct Templates","text":"<p>You will need to construct your own BEAST2 templates based on the analysis that you would like to do. If you want to see two examples, the package provides two templates for fixed and estimated tree analyses. You can download them directly from GitHub:</p> <ul> <li>Alignment based Template: Template.xml</li> <li>Fixed Tree based Template: Template_fixedtree.xml</li> </ul>"},{"location":"#step-2-generate-xml","title":"Step 2: Generate XML","text":"<p>Use the <code>generate_xml</code> wrapper function.</p> <p>Scenario A: Standard Analysis (Tree will be estimated) Requires both a Nexus alignment file and a Tree file from ReMASTER.</p> <pre><code>from beast2analysisutils.remaster import generate_xml\n\ngenerate_xml(\n    template_path=\"./Template.xml\",\n    tree_path=\"simulation.trees\",      # ReMASTER tree output\n    output_path=\"output.xml\",\n    alignment_path=\"simulation.nexus\", # ReMASTER alignment output\n    fixed_tree=False                   # Default\n)\n</code></pre> <p>Scenario B: Fixed Tree Analysis Uses the simulated tree directly in the XML. Nexus file is optional (dummy sequences will be generated if missing).</p> <pre><code>from beast2analysisutils.remaster import generate_xml\n\ngenerate_xml(\n    template_path=\"./Template_fixedtree.xml\",\n    tree_path=\"simulation.trees\",\n    output_path=\"output_fixed.xml\",\n    alignment_path=None,               # Optional for fixed tree\n    fixed_tree=True\n)\n</code></pre>"},{"location":"#2-ess-analysis","title":"2. ESS Analysis","text":"<p>Calculate ESS for all parameters in a BEAST2 log file to check for convergence.</p> <pre><code>from beast2analysisutils.ess import analyze_ess\n\n# Calculate ESS for all numeric columns\ness_df = analyze_ess(\n    log_source=\"beast_output.log\",\n    output_path=\"ess_results.csv\",\n    burnin=0.1,             # 10% burn-in\n    check_threshold=True    # Check if ESS &gt; 200 for key parameters\n)\n\nprint(ess_df.head())\n</code></pre>"},{"location":"#detailed-documentation","title":"Detailed Documentation","text":""},{"location":"#remaster-to-beast2-xml-workflow","title":"ReMASTER to BEAST2 XML Workflow","text":"<p>This package provides functionality to generate BEAST2 XML configuration files by combining a template XML with simulation data.</p>"},{"location":"#wrapper-function-generate_xml","title":"Wrapper Function: <code>generate_xml</code>","text":"<p>The <code>generate_xml</code> function is the main entry point. It performs the following steps: 1.  Validates Inputs: Checks for required files based on <code>fixed_tree</code> mode. 2.  Extracts Data: Parses sequences, leaf dates, and types from input files. 3.  Consistency Check: Verifies that the number of sequences matches the number of tree leaves. 4.  Prints Stats: Displays sequence counts, leaf counts, trait distribution, and date ranges. 5.  Generates XML: Fills the template with the processed data.</p>"},{"location":"#xml-template-requirements","title":"XML Template Requirements","text":"<p>Your input template XML must be a valid BEAST2 XML file containing specific placeholders:</p> <ol> <li><code>INSERTSEQUENCES</code>: Replaced with <code>&lt;sequence&gt;</code> blocks.</li> <li><code>INSERTTRAITDATES</code>: (not needed for fixed tree) Replaced with <code>taxon=date</code> pairs (YYYY/MM/DD).</li> <li><code>INSERTTRAITTYPES</code>: Replaced with <code>taxon=type</code> pairs.</li> <li><code>INSERTNEWICKTREE</code>: (Fixed tree only) Replaced with the Newick tree string.</li> </ol> <p>Example Template Structure:</p> <pre><code>&lt;data id=\"alignment\" spec=\"Alignment\"&gt;\n    INSERTSEQUENCES\n&lt;/data&gt;\n\n&lt;trait id=\"dateTrait\" spec=\"beast.base.evolution.tree.TraitSet\" traitname=\"date\" value=\"INSERTTRAITDATES\"&gt;\n    ...\n&lt;/trait&gt;\n\n&lt;trait id=\"typeTrait\" spec=\"mascot.util.InitializedTraitSet\" traitname=\"type\" value=\"INSERTTRAITTYPES\"&gt;\n    ...\n&lt;/trait&gt;\n\n&lt;!-- For Fixed Tree templates --&gt;\n&lt;init id=\"NewickTree\" spec=\"beast.base.evolution.tree.TreeParser\" newick=\"INSERTNEWICKTREE\" .../&gt;\n</code></pre>"},{"location":"#time-and-date-interpretation","title":"Time and Date Interpretation","text":"<p>ReMASTER simulations typically output trees with a <code>time</code> attribute (years since simulation start). The package converts these to calendar dates using a reference <code>start_date</code> (default \"2000/01/01\"). leaf_date = <code>start_date</code> + <code>time</code> (converted to days).</p>"},{"location":"reference/","title":"Package Function Reference","text":""},{"location":"reference/#effective-sample-size-ess","title":"Effective Sample Size (ESS)","text":"<p>This module provides utilities to read BEAST log files, calculate ESS using autocorrelation analysis, and report on convergence thresholds.</p> <p>Calculate Effective Sample Size (ESS) from BEAST log files.</p> <p>This module provides utilities to read BEAST log files, calculate ESS using autocorrelation analysis, and report on convergence thresholds.</p>"},{"location":"reference/#beast2analysisutils.ess.analyze_ess","title":"<code>analyze_ess(log_source, output_path, burnin=0.1, check_threshold=True)</code>","text":"<p>Calculate ESS for a BEAST log file or DataFrame and save results.</p> <p>Parameters:</p> Name Type Description Default <code>log_source</code> <code>Union[str, DataFrame]</code> <p>Path to the BEAST log file or a pandas DataFrame.</p> required <code>output_path</code> <code>str</code> <p>Path to save the CSV results table.</p> required <code>burnin</code> <code>float</code> <p>Fraction of samples to discard as burn-in (0.0 to 1.0), default 0.1.</p> <code>0.1</code> <code>check_threshold</code> <code>bool</code> <p>If True, prints a report on how many samples were needed to reach ESS &gt; 200 for 'posterior', 'prior', and 'likelihood' columns (if present aka intersection).</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the calculated ESS values for each parameter.</p> Source code in <code>src/beast2analysisutils/ess.py</code> <pre><code>def analyze_ess(\n    log_source: Union[str, pd.DataFrame],\n    output_path: str,\n    burnin: float = 0.1,\n    check_threshold: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate ESS for a BEAST log file or DataFrame and save results.\n\n    Args:\n        log_source: Path to the BEAST log file or a pandas DataFrame.\n        output_path: Path to save the CSV results table.\n        burnin: Fraction of samples to discard as burn-in (0.0 to 1.0), default 0.1.\n        check_threshold: If True, prints a report on how many samples were needed to reach ESS &gt; 200\n            for 'posterior', 'prior', and 'likelihood' columns (if present aka intersection).\n\n    Returns:\n        pd.DataFrame: DataFrame containing the calculated ESS values for each parameter.\n    \"\"\"\n    # 1. Load Data\n    if isinstance(log_source, str):\n        # BEAST logs are typically tab-separated with '#' comments\n        try:\n            df = pd.read_csv(log_source, sep=\"\\t\", comment=\"#\")\n        except Exception as e:\n            raise ValueError(f\"Failed to read log file {log_source}: {e}\")\n    elif isinstance(log_source, pd.DataFrame):\n        df = log_source\n    else:\n        raise TypeError(\"log_source must be a file path (str) or pandas DataFrame\")\n\n    n_samples = len(df)\n    if n_samples == 0:\n        raise ValueError(\"Input data is empty\")\n\n    # 2. Apply Burn-in\n    if not 0 &lt;= burnin &lt; 1:\n        raise ValueError(\"Burn-in must be in range [0, 1)\")\n\n    burnin_idx = int(n_samples * burnin)\n    df_burnin = df.iloc[burnin_idx:].reset_index(drop=True)\n\n    print(\n        f\"Total samples: {n_samples}. After {burnin * 100:.1f}% burn-in: {len(df_burnin)} samples.\"\n    )\n\n    # 3. Calculate ESS for numeric columns\n    # Exclude 'sample' or 'Sample' column if present, though effective_sample_size handles linear trend okay,\n    # usually we skip it.\n    skip_cols = {\"sample\", \"Sample\", \"state\", \"State\"}\n    numeric_cols = df_burnin.select_dtypes(include=[np.number]).columns\n\n    results = []\n    for col in numeric_cols:\n        if col in skip_cols:\n            continue\n\n        # Calculate ESS\n        val = effective_sample_size(df_burnin[col].values)\n        results.append({\"Parameter\": col, \"ESS\": val})\n\n    results_df = pd.DataFrame(results).sort_values(\"ESS\")\n\n    # 4. Save Results\n    results_df.to_csv(output_path, index=False)\n    print(f\"ESS results saved to {output_path}\")\n\n    # 5. Check Thresholds (Optional)\n    if check_threshold:\n        target_cols = set([\"posterior\", \"prior\", \"likelihood\"])\n        # Case insensitive matching to be robust? The user used exact casing in their snippet.\n        # But let's check exact match first as per user snippet.\n        cols_to_check = target_cols.intersection(df.columns)\n\n        if cols_to_check:\n            print(\"\\nFinding samples needed for ESS &gt; 200:\")\n            threshold = 200.0\n            for col in cols_to_check:\n                data_series = df[col].values\n                n_needed = find_ess_threshold(data_series, threshold=threshold)\n\n                if n_needed is not None:\n                    print(f\"{col}: ESS &gt; {threshold} achieved after {n_needed} samples\")\n                else:\n                    final_ess = effective_sample_size(df_burnin[col].values)\n                    print(\n                        f\"{col}: ESS &gt; {threshold} not achieved (final ESS: {final_ess:.2f})\"\n                    )\n        else:\n            print(\n                \"\\ncheck_threshold=True but 'posterior', 'prior', or 'likelihood' columns not found.\"\n            )\n\n    return results_df\n</code></pre>"},{"location":"reference/#beast2analysisutils.ess.effective_sample_size","title":"<code>effective_sample_size(x, max_lag=None)</code>","text":"<p>Calculate effective sample size using autocorrelation.</p> <ul> <li>Demeans the data</li> <li>Calculates autocorrelation up to max_lag</li> <li>Sums positive autocorrelations</li> <li>ESS = N / (1 + 2 * sum)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, List[float]]</code> <p>Time series data.</p> required <code>max_lag</code> <code>Optional[int]</code> <p>Maximum lag for autocorrelation calculation. If None, uses min(N-1, 10000) where N is the length of x.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Effective sample size.</p> Source code in <code>src/beast2analysisutils/ess.py</code> <pre><code>def effective_sample_size(\n    x: Union[np.ndarray, List[float]], max_lag: Optional[int] = None\n) -&gt; float:\n    \"\"\"\n    Calculate effective sample size using autocorrelation.\n\n    - Demeans the data\n    - Calculates autocorrelation up to max_lag\n    - Sums positive autocorrelations\n    - ESS = N / (1 + 2 * sum)\n\n    Args:\n        x: Time series data.\n        max_lag: Maximum lag for autocorrelation calculation. If None, uses\n            min(N-1, 10000) where N is the length of x.\n\n    Returns:\n        float: Effective sample size.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    N = len(x)\n\n    if N &lt; 2:\n        return 0.0\n\n    # Demean the data\n    x_demeaned = x - np.mean(x)\n\n    # Set max_lag if not provided\n    if max_lag is None:\n        max_lag = min(N - 1, 10000)\n    else:\n        max_lag = min(max_lag, N - 1)\n\n    # Calculate autocorrelation function (matching R's acf with type=\"correlation\")\n    # Autocorrelation at lag k: r_k = Cov(X_t, X_{t+k}) / Var(X)\n    # Calculate Var(X)\n    variance = np.var(x_demeaned, ddof=0)\n    if variance == 0:\n        return float(\"inf\")  # Constant series has infinite ESS\n\n    # Calculate autocorrelations\n    acf_lags = []\n    for lag in range(1, max_lag + 1):\n        # Optimized covariance calculation using slice\n        # Cov(X_t, X_{t+k}) = mean((X_t - mu)(X_{t+k} - mu))\n        covariance = np.sum(x_demeaned[:-lag] * x_demeaned[lag:]) / (N - lag)\n        acf_val = covariance / variance\n        if acf_val &lt;= 0:\n            break\n        acf_lags.append(acf_val)\n\n    # Calculate ESS\n    pos_sum = sum(acf_lags)\n    ess = N / (1 + 2 * pos_sum)\n\n    return ess\n</code></pre>"},{"location":"reference/#beast2analysisutils.ess.find_ess_threshold","title":"<code>find_ess_threshold(data, threshold=200, max_lag=None)</code>","text":"<p>Find the minimum number of samples needed to achieve ESS &gt;= threshold.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[ndarray, List[float]]</code> <p>Logged sequential data.</p> required <code>threshold</code> <code>float</code> <p>ESS threshold to achieve.</p> <code>200</code> <code>max_lag</code> <code>Optional[int]</code> <p>Maximum lag for autocorrelation calculation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>int or None: Minimum number of samples needed to achieve ESS &gt;= threshold, or None if threshold not reached.</p> Source code in <code>src/beast2analysisutils/ess.py</code> <pre><code>def find_ess_threshold(\n    data: Union[np.ndarray, List[float]],\n    threshold: float = 200,\n    max_lag: Optional[int] = None,\n) -&gt; Optional[int]:\n    \"\"\"\n    Find the minimum number of samples needed to achieve ESS &gt;= threshold.\n\n    Args:\n        data: Logged sequential data.\n        threshold: ESS threshold to achieve.\n        max_lag: Maximum lag for autocorrelation calculation.\n\n    Returns:\n        int or None: Minimum number of samples needed to achieve ESS &gt;= threshold,\n            or None if threshold not reached.\n    \"\"\"\n    data = np.asarray(data)\n    N = len(data)\n\n    # Check progressively larger sample sizes\n    for n in range(100, N + 1, 100):\n        ess = effective_sample_size(data[:n], max_lag)\n        if ess &gt;= threshold:\n            # Refine search in smaller increments\n            for n_refined in range(max(100, n - 100), n + 1, 10):\n                if effective_sample_size(data[:n_refined], max_lag) &gt;= threshold:\n                    return n_refined\n            return n\n\n    # Check full dataset\n    if effective_sample_size(data, max_lag) &gt;= threshold:\n        return N\n\n    return None\n</code></pre>"},{"location":"reference/#beast2analysisutils.ess.read_log_file","title":"<code>read_log_file(filename)</code>","text":"<p>Read BEAST log file and extract data.</p> <p>This function parses the standard BEAST .log format, ignoring comments.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to log file.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], ndarray]</code> <p>Tuple[List[str], np.ndarray]: A tuple containing: - header: Column names - data: Data matrix (samples x parameters)</p> Source code in <code>src/beast2analysisutils/ess.py</code> <pre><code>def read_log_file(filename: str) -&gt; Tuple[List[str], np.ndarray]:\n    \"\"\"\n    Read BEAST log file and extract data.\n\n    This function parses the standard BEAST .log format, ignoring comments.\n\n    Args:\n        filename: Path to log file.\n\n    Returns:\n        Tuple[List[str], np.ndarray]: A tuple containing:\n            - header: Column names\n            - data: Data matrix (samples x parameters)\n    \"\"\"\n    header = None\n    data_rows = []\n\n    with open(filename, \"r\") as f:\n        for line in f:\n            line = line.strip()\n            if line.startswith(\"#\"):\n                continue\n\n            if header is None:\n                header = line.split(\"\\t\")\n                continue\n\n            if line:\n                values = line.split(\"\\t\")\n                try:\n                    row = [float(v) for v in values]\n                    data_rows.append(row)\n                except ValueError:\n                    continue\n\n    if header is None:\n        raise ValueError(\"No header found in log file\")\n\n    if len(data_rows) == 0:\n        raise ValueError(\"No data rows found in log file\")\n\n    data = np.array(data_rows)\n\n    if data.shape[1] != len(header):\n        raise ValueError(\n            f\"Data columns ({data.shape[1]}) don't match header ({len(header)})\"\n        )\n\n    return header, data\n</code></pre>"},{"location":"reference/#remaster-xml-generation","title":"ReMASTER XML Generation","text":"<p>This module handles the extraction of data from ReMASTER simulations (Nexus alignment and tree files) and the generation of BEAST2 XML files from templates.</p>"},{"location":"reference/#beast2analysisutils.remaster.collapse_single_child_nodes","title":"<code>collapse_single_child_nodes(clade)</code>","text":"<p>Recursively collapse nodes with a single child, adding branch lengths.</p> Source code in <code>src/beast2analysisutils/remaster.py</code> <pre><code>def collapse_single_child_nodes(clade):\n    \"\"\"\n    Recursively collapse nodes with a single child, adding branch lengths.\n    \"\"\"\n    # If the clade is terminal, return it unchanged\n    if clade.is_terminal():\n        return clade\n\n    # Process children first (postorder traversal)\n    new_clades = []\n    for child in clade.clades:\n        simplified_child = collapse_single_child_nodes(child)\n        new_clades.append(simplified_child)\n\n    clade.clades = new_clades\n\n    # If this node has only one child, collapse it\n    if len(clade.clades) == 1:\n        child = clade.clades[0]\n        # Add this clade's branch length to the child's branch length\n        child.branch_length = (child.branch_length or 0) + (clade.branch_length or 0)\n        return child  # Return the collapsed child\n\n    # If this node has more than one child, keep it\n    return clade\n</code></pre>"},{"location":"reference/#beast2analysisutils.remaster.convert_times_to_dates","title":"<code>convert_times_to_dates(times, start_date='2000/01/01')</code>","text":"<p>Converts relative simulation times (in years) to date strings based on a start date.</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>dict</code> <p>{taxon_id: relative_time_float}</p> required <code>start_date</code> <code>str</code> <p>Format \"YYYY/MM/DD\", defaults to \"2000/01/01\".</p> <code>'2000/01/01'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>{taxon_id: date_string} where date_string is \"YYYY/MM/DD\".</p> Source code in <code>src/beast2analysisutils/remaster.py</code> <pre><code>def convert_times_to_dates(times, start_date=\"2000/01/01\"):\n    \"\"\"\n    Converts relative simulation times (in years) to date strings based on a start date.\n\n    Args:\n        times (dict): {taxon_id: relative_time_float}\n        start_date (str): Format \"YYYY/MM/DD\", defaults to \"2000/01/01\".\n\n    Returns:\n        dict: {taxon_id: date_string} where date_string is \"YYYY/MM/DD\".\n    \"\"\"\n    start_date_dt = datetime.strptime(start_date, \"%Y/%m/%d\")\n    dates = {}\n\n    # Logic extracted from user snippet:\n    # rel_time is in years, convert to days\n\n    for taxon, rel_time in times.items():\n        rel_time_days = int(float(rel_time) * 365)\n        new_date = start_date_dt + timedelta(days=rel_time_days)\n        dates[taxon] = new_date.strftime(\"%Y/%m/%d\")\n\n    return dates\n</code></pre>"},{"location":"reference/#beast2analysisutils.remaster.extract_remaster_data","title":"<code>extract_remaster_data(alignment_path, tree_path)</code>","text":"<p>Extracts sequences, leaf dates, and leaf types from ReMASTER output files.</p> <p>Parameters:</p> Name Type Description Default <code>alignment_path</code> <code>str or None</code> <p>Path to the Nexus alignment file. If None, sequences will be artificial (empty/placeholder).</p> required <code>tree_path</code> <code>str</code> <p>Path to the Nexus tree file.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(sequences, dates, types, newick_tree) - sequences (dict): {taxon_id: sequence_string} - dates (dict): {taxon_id: date_float_or_string} - types (dict): {taxon_id: type_string} - newick_tree (str): The tree as a Newick string.</p> Source code in <code>src/beast2analysisutils/remaster.py</code> <pre><code>def extract_remaster_data(alignment_path, tree_path):\n    \"\"\"\n    Extracts sequences, leaf dates, and leaf types from ReMASTER output files.\n\n    Args:\n        alignment_path (str or None): Path to the Nexus alignment file. If None, sequences will be artificial (empty/placeholder).\n        tree_path (str): Path to the Nexus tree file.\n\n    Returns:\n        tuple: (sequences, dates, types, newick_tree)\n            - sequences (dict): {taxon_id: sequence_string}\n            - dates (dict): {taxon_id: date_float_or_string}\n            - types (dict): {taxon_id: type_string}\n            - newick_tree (str): The tree as a Newick string.\n    \"\"\"\n    # 1. Parse Alignment\n    sequences = {}\n    if alignment_path:\n        alignment = AlignIO.read(alignment_path, \"nexus\")\n        for record in alignment:\n            sequences[record.id] = str(record.seq).lower()\n    else:\n        # If no alignment, we will populate dummy sequences based on tree leaves later\n        pass\n\n    # 2. Parse Tree and extract metadata\n    # We need to handle the potential trailing comma in TRANSLATE block issue mentioned in original script\n    # For now, let's try reading directly with dendropy, if it fails, we implement the fix.\n    # The original script implemented a fix for 'TRANSLATE' block trailing comma. \n    # Let's incorporate that fix to be safe.\n\n    with open(tree_path, \"r\") as f:\n        lines = f.readlines()\n\n    in_translate = False\n    for i, line in enumerate(lines):\n        if line.strip().lower().startswith(\"translate\"):\n            in_translate = True\n            continue\n        if in_translate:\n            if \";\" in line:\n                prev_idx = i - 1\n                while prev_idx &gt; 0 and lines[prev_idx].strip() == \"\":\n                    prev_idx -= 1\n                if prev_idx &gt; 0:\n                    prev_line = lines[prev_idx].rstrip(\"\\n\")\n                    if prev_line.rstrip().endswith(\",\"):\n                        lines[prev_idx] = prev_line.rstrip(\",\\n\") + \"\\n\"\n                in_translate = False\n                break\n\n    # Use io.StringIO to avoid writing temp file to disk if possible, \n    # but dendropy .get(path=...) or .get(data=...)\n    tree_content = \"\".join(lines)\n    tree = dendropy.Tree.get(\n        data=tree_content, schema=\"nexus\", preserve_underscores=True\n    )\n\n    times = {}\n    types = {}\n\n    for leaf in tree.leaf_node_iter():\n        real_label = leaf.taxon.label\n\n        # Extract annotations\n        # ReMASTER/BEAST2 annotations usually in [&amp;type=\"...\", time=...]\n        # dendropy parses these into annotations\n\n        if leaf.annotations:\n            # Note: The original script accessed 'type' and 'time'.\n            # Let's try to get them safely.\n            type_val = leaf.annotations.get_value(\"type\")\n            time_val = leaf.annotations.get_value(\"time\")\n\n            if type_val:\n                # Remove curly braces if present (some versions of dendropy/remaster might keep them?)\n                # The original script did .replace('{','').replace('}','')\n                types[real_label] = str(type_val).replace('{','').replace('}','')\n\n            if time_val is not None:\n                times[real_label] = float(time_val)\n\n    # If sequences dict is empty (no alignment provided), fill with dummy\n    if not sequences:\n        # Just use \"n\" as dummy sequence for each taxon found in tree\n        # Length doesn't matter much for fixed tree if not used for likelihood, \n        # but BEAST XML usually requires some sequence data.\n        # However, for fixed tree, often we don't have sequences? \n        # User requirement: \"sequences still need to be placed but they are artifical\"\n        for taxon in times.keys():\n            sequences[taxon] = \"n\"\n\n    # Also extract the raw Newick tree string for fixed tree templates\n    with open(tree_path, \"r\") as f:\n        tree_text = f.read()\n\n    # We use the custom parser to get a clean Newick string suitable for TreeParser\n    try:\n        raw_newick_tree = get_state0_newick(tree_text)\n\n        # Collapse single child nodes\n        tree_obj = Phylo.read(StringIO(raw_newick_tree), \"newick\")\n        tree_obj.root = collapse_single_child_nodes(tree_obj.root)\n        newick_tree = tree_obj.format(\"newick\").strip()\n\n    except Exception as e:\n        # If extraction fails (e.g. unexpected format), we might not have a tree string\n        # This is okay if we aren't using a fixed tree template, but let's warn or return None\n        print(f\"Warning: Could not extract Newick tree string: {e}\")\n        newick_tree = None\n\n    return sequences, times, types, newick_tree\n</code></pre>"},{"location":"reference/#beast2analysisutils.remaster.fill_template","title":"<code>fill_template(template_path, output_path, sequences, dates, types, newick_tree=None)</code>","text":"<p>Fills the BEAST2 XML template with the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>Path to the template XML file.</p> required <code>output_path</code> <code>str</code> <p>Path to save the generated XML file.</p> required <code>sequences</code> <code>dict</code> <p>{taxon_id: sequence_string}</p> required <code>dates</code> <code>dict</code> <p>{taxon_id: date_value_string} - properly formatted dates e.g. YYYY/MM/DD</p> required <code>types</code> <code>dict</code> <p>{taxon_id: type_value}</p> required <code>newick_tree</code> <code>str</code> <p>The Newick tree string to insert (for fixed tree templates).</p> <code>None</code> Source code in <code>src/beast2analysisutils/remaster.py</code> <pre><code>def fill_template(template_path, output_path, sequences, dates, types, newick_tree=None):\n    \"\"\"\n    Fills the BEAST2 XML template with the provided data.\n\n    Args:\n        template_path (str): Path to the template XML file.\n        output_path (str): Path to save the generated XML file.\n        sequences (dict): {taxon_id: sequence_string}\n        dates (dict): {taxon_id: date_value_string} - properly formatted dates e.g. YYYY/MM/DD\n        types (dict): {taxon_id: type_value}\n        newick_tree (str, optional): The Newick tree string to insert (for fixed tree templates).\n    \"\"\"\n\n    # Read template content\n    with open(template_path, \"r\") as f:\n        template_content = f.read()\n\n    # Validate date formats\n    for taxon, date_str in dates.items():\n        try:\n            # Check for the primary format we use\n            datetime.strptime(date_str, \"%Y/%m/%d\")\n        except ValueError:\n            raise ValueError(f\"Invalid date format for taxon '{taxon}': '{date_str}'. Expected format YYYY/MM/DD.\")\n\n    # 1. Prepare Sequences XML Block\n    # Format: &lt;sequence id=\"seq_{taxon}\" spec=\"Sequence\" taxon=\"{taxon}\" totalcount=\"4\" value=\"{seq}\"/&gt;\n    seq_lines = []\n    # Sort keys for deterministic output\n    for taxon in sorted(sequences.keys()):\n        seq = sequences[taxon]\n        line = f'    &lt;sequence id=\"seq_{taxon}\" spec=\"Sequence\" taxon=\"{taxon}\" totalcount=\"4\" value=\"{seq}\"/&gt;'\n        seq_lines.append(line)\n\n    sequences_xml_block = \"\\n\".join(seq_lines)\n\n    # 2. Prepare Dates String\n    # Format: taxon=date,taxon=date...\n    date_strings = []\n    for taxon in sorted(dates.keys()):\n        date_strings.append(f\"{taxon}={dates[taxon]}\")\n    dates_value = \",\".join(date_strings)\n\n    # 3. Prepare Types String\n    # Format: taxon=type,taxon=type...\n    type_strings = []\n    for taxon in sorted(types.keys()):\n        type_strings.append(f\"{taxon}={types[taxon]}\")\n    types_value = \",\".join(type_strings)\n\n    # 4. Perform Replacements\n    # We use simple string replacement as requested for specific placeholders\n    # It is safer than parsing XML if the placeholders are just text markers inside tags\n    # typically these placeholders are inside value=\"...\" attributes or just as text content.\n\n    new_content = template_content\n\n    if \"INSERTSEQUENCES\" in new_content:\n        new_content = new_content.replace(\"INSERTSEQUENCES\", sequences_xml_block)\n\n    if \"INSERTTRAITDATES\" in new_content:\n        # Only replace if we have headers; otherwise maybe we should remove the block?\n        # But commonly we just put the dates.\n        new_content = new_content.replace(\"INSERTTRAITDATES\", dates_value)\n\n    if \"INSERTTRAITTYPES\" in new_content:\n        new_content = new_content.replace(\"INSERTTRAITTYPES\", types_value)\n\n    if \"INSERTNEWICKTREE\" in new_content:\n        if newick_tree:\n            new_content = new_content.replace(\"INSERTNEWICKTREE\", newick_tree)\n        else:\n            raise ValueError(\"Template requires INSERTNEWICKTREE but no tree string was provided.\")\n\n    # Write output\n    with open(output_path, \"w\") as f:\n        f.write(new_content)\n</code></pre>"},{"location":"reference/#beast2analysisutils.remaster.generate_xml","title":"<code>generate_xml(template_path, tree_path, output_path, alignment_path=None, fixed_tree=False, start_date='2000/01/01')</code>","text":"<p>Wrapper function to generate BEAST2 XML from ReMASTER output.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>Path to the XML template.</p> required <code>tree_path</code> <code>str</code> <p>Path to the ReMASTER tree file.</p> required <code>output_path</code> <code>str</code> <p>Path where the final XML will be saved.</p> required <code>alignment_path</code> <code>str</code> <p>Path to the ReMASTER nexus alignment file. Required if fixed_tree is False.</p> <code>None</code> <code>fixed_tree</code> <code>bool</code> <p>Whether to use fixed tree mode.</p> <code>False</code> <code>start_date</code> <code>str</code> <p>Start date for time conversion (YYYY/MM/DD). Defaults to \"2000/01/01\".</p> <code>'2000/01/01'</code> Source code in <code>src/beast2analysisutils/remaster.py</code> <pre><code>def generate_xml(template_path, tree_path, output_path, alignment_path=None, fixed_tree=False, start_date=\"2000/01/01\"):\n    \"\"\"\n    Wrapper function to generate BEAST2 XML from ReMASTER output.\n\n    Args:\n        template_path (str): Path to the XML template.\n        tree_path (str): Path to the ReMASTER tree file.\n        output_path (str): Path where the final XML will be saved.\n        alignment_path (str, optional): Path to the ReMASTER nexus alignment file. Required if fixed_tree is False.\n        fixed_tree (bool): Whether to use fixed tree mode.\n        start_date (str): Start date for time conversion (YYYY/MM/DD). Defaults to \"2000/01/01\".\n    \"\"\"\n\n    # Validation\n    if not fixed_tree and alignment_path is None:\n        raise ValueError(\"alignment_path is required when fixed_tree is False.\")\n\n    # Extract data\n    # extract_remaster_data now handles None alignment_path by generating dummy sequences\n    sequences, times, types, newick_tree = extract_remaster_data(alignment_path, tree_path)\n\n    # Print Info\n    print(\"--- Remaster Wrapper Info ---\")\n    if alignment_path:\n        print(f\"Number of sequences in nexus file: {len(sequences)}\")\n    else:\n        print(f\"Number of sequences (artificial): {len(sequences)}\")\n\n    print(f\"Number of leaves in tree: {len(times)}\")\n\n    # Verification of consistency\n    if len(sequences) != len(times):\n        print(f\"WARNING: Mismatch between number of sequences ({len(sequences)}) and tree leaves ({len(times)}).\")\n\n    # Iterate type trait stats\n    trait_counts = {}\n    for taxon, trait in types.items():\n        trait_counts[trait] = trait_counts.get(trait, 0) + 1\n\n    print(\"\\nLeaves per type trait:\")\n    for trait, count in trait_counts.items():\n        print(f\"  {trait}: {count}\")\n\n    # Date info\n    if not fixed_tree:\n        if times:\n            min_time = min(times.values())\n            max_time = max(times.values())\n            # Convert to dates for display\n            # times are relative years from root usually? Or simply floats.\n            # convert_times_to_dates logic: start_date + relative_time\n            # Assuming 'times' from extract_remaster_data are appropriate for this conversion.\n            # Let's show the converted range.\n\n            # Temporary internal conversion for display\n            temp_dates = convert_times_to_dates(times, start_date)\n            date_values = [datetime.strptime(d, \"%Y/%m/%d\") for d in temp_dates.values()]\n            min_date = min(date_values).strftime(\"%Y/%m/%d\")\n            max_date = max(date_values).strftime(\"%Y/%m/%d\")\n\n            print(f\"\\nDate Range: {min_date} to {max_date}\")\n        else:\n            print(\"\\nDate Range: N/A (no times found)\")\n    else:\n        print(\"\\nDate Range: Ignored (Fixed Tree mode)\")\n\n    print(\"-----------------------------\")\n\n    # Process Data\n    real_dates = convert_times_to_dates(times, start_date)\n\n    # Generate XML\n    fill_template(\n        template_path, \n        output_path, \n        sequences, \n        real_dates, \n        types, \n        newick_tree=newick_tree if fixed_tree else None\n    )\n    print(f\"\\nSuccessfully generated XML at: {output_path}\")\n</code></pre>"},{"location":"reference/#beast2analysisutils.remaster.get_state0_newick","title":"<code>get_state0_newick(nexus_text)</code>","text":"<p>Given the full text of a NEXUS file, extract the tree (STATE_0 or TREE) as a Newick string with integer IDs replaced by leaf names.</p> Source code in <code>src/beast2analysisutils/remaster.py</code> <pre><code>def get_state0_newick(nexus_text):\n    \"\"\"\n    Given the full text of a NEXUS file, extract the tree (STATE_0 or TREE)\n    as a Newick string with integer IDs replaced by leaf names.\n    \"\"\"\n\n    # 1. Extract the TRANSLATE block to build a dictionary:\n    #    numeric ID =&gt; leaf name (if present)\n    translate_block_pattern = re.compile(\n        r\"(?is)Begin\\s+trees\\s*;.*?Translate\\s*(.*?)\\s*;\", re.DOTALL\n    )\n    translate_match = translate_block_pattern.search(nexus_text)\n    translation_dict = {}\n    if translate_match:\n        translate_block = translate_match.group(1)\n        # Build a dictionary from the translate block\n        for line in translate_block.split(\"\\n\"):\n            line = line.strip().rstrip(\",;\")\n            if not line:\n                continue\n            parts = line.split(None, 1)\n            if len(parts) == 2:\n                num_id, label = parts\n                translation_dict[num_id] = label\n    else:\n        # No Translate block - will use numeric IDs directly\n        pass\n\n    # 2. Extract the tree STATE_0 or TREE = ( ... );\n    #    We'll match up to the semicolon\n    state0_pattern = re.compile(r\"tree\\s+STATE_0\\s*=\\s*(.*?)\\s*;\", re.DOTALL)\n    state0_match = state0_pattern.search(nexus_text)\n\n    if not state0_match:\n        # Try TREE instead\n        state0_pattern = re.compile(r\"tree\\s+TREE\\s*=\\s*(.*?)\\s*;\", re.DOTALL)\n        state0_match = state0_pattern.search(nexus_text)\n\n    if not state0_match:\n        raise ValueError(\n            \"Could not find 'tree STATE_0 = ...;' or 'tree TREE = ...;' in the NEXUS text.\"\n        )\n\n    tree_str = state0_match.group(1).strip()\n\n    # 3. Remove bracketed metadata like [&amp;type=\"D0\",time=0.0]\n    tree_str = re.sub(r\"\\[.*?\\]\", \"\", tree_str)\n\n    # 4. Safely replace only integer IDs with leaf names\n    #    (leave floating-point numbers alone)\n    def replace_labels_safely(s, dictionary):\n        result = []\n        i = 0\n        while i &lt; len(s):\n            c = s[i]\n            if c.isdigit():\n                j = i + 1\n                while j &lt; len(s) and s[j].isdigit():\n                    j += 1\n                numeric_token = s[i:j]\n                if j &lt; len(s) and s[j] == \":\":\n                    if numeric_token in dictionary:\n                        result.append(dictionary[numeric_token])\n                    else:\n                        result.append(numeric_token)\n                else:\n                    result.append(numeric_token)\n                i = j\n            else:\n                result.append(c)\n                i += 1\n        return \"\".join(result)\n\n    tree_str = replace_labels_safely(tree_str, translation_dict)\n\n    # 5. Remove extraneous whitespace, then return\n    tree_str = re.sub(r\"\\s+\", \"\", tree_str)\n\n    # Return a valid Newick with trailing semicolon\n    return tree_str + \";\"\n</code></pre>"}]}